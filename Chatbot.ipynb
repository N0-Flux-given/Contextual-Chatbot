{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37432bit0bab2797a6d74ec0b66b64f8d195f925",
   "display_name": "Python 3.7.4 32-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "source": [
    "# Contextual chatbot - Pytorch\n",
    "\n",
    "\n",
    "## The idea:\n",
    "\n",
    "### There is going to be a pre-defined set of categories \n",
    "### Once user enters text, the job of the model is to predict which category the input belongs to \n",
    "### Then, simply pick a random response from the category's pre-defined responses to reply"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# <img src=\"pics/idea.jpg\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "portStemmer = PorterStemmer()\n",
    "def tokenize(sentence):\n",
    "    return nltk.word_tokenize(sentence)\n",
    "\n",
    "\n",
    "def stem(word):    \n",
    "    return portStemmer.stem(word.lower())\n",
    "\n",
    "\n",
    "def bag_words(input_sentence, all_words):\n",
    "    sentence = [stem(w) for w in input_sentence]    \n",
    "    bag = np.zeros(len(all_words), dtype=np.float32)\n",
    "    for i, w in enumerate(all_words):        \n",
    "        if w in sentence:            \n",
    "            bag[i] = 1.0\n",
    "    return bag\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'work'"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "stem(\"works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 1.], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "total_words = ['this', 'is', 'how', 'stemming', 'works']    # all words \n",
    "input_words = ['how', 'works']                              # how many words from this exist in all words \n",
    "bag_words(input_words, [stem(x) for x in total_words])"
   ]
  },
  {
   "source": [
    "## Preparing the training data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"categories.json\", \"r\") as f:\n",
    "    intents = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "tags = []\n",
    "pattern_tags = []\n",
    "for intent in intents[\"intents\"]:\n",
    "    tag = intent[\"tag\"]\n",
    "    tags.append(tag)\n",
    "    pattern_tag = []\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        tokenized_pattern = tokenize(pattern)\n",
    "        all_words.extend(tokenized_pattern)\n",
    "        pattern_tags.append((tokenized_pattern, tag))\n",
    "\n",
    "ignore_chars = [\"?\", \"!\", \".\", \",\"]\n",
    "\n"
   ]
  },
  {
   "source": [
    "# <img src=\"pics/json_iteration.jpg\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Hi', 'Hey', 'How', 'are', 'you', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Bye', 'See', 'you', 'later', 'Goodbye', 'Thanks', 'Thank', 'you', 'That', \"'s\", 'helpful', 'Thank', \"'s\", 'a', 'lot', '!', 'Which', 'items', 'do', 'you', 'have', '?', 'What', 'kinds', 'of', 'items', 'are', 'there', '?', 'What', 'do', 'you', 'sell', '?', 'Do', 'you', 'take', 'credit', 'cards', '?', 'Do', 'you', 'accept', 'Mastercard', '?', 'Can', 'I', 'pay', 'with', 'Paypal', '?', 'Are', 'you', 'cash', 'only', '?', 'How', 'long', 'does', 'delivery', 'take', '?', 'How', 'long', 'does', 'shipping', 'take', '?', 'When', 'do', 'I', 'get', 'my', 'delivery', '?', 'Tell', 'me', 'a', 'joke', '!', 'Tell', 'me', 'something', 'funny', '!', 'Do', 'you', 'know', 'a', 'joke', '?']\n"
     ]
    }
   ],
   "source": [
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying stemming and remove duplicate words by type-casting the list into a set\n",
    "all_words = set([stem(w) for w in all_words if w not in ignore_chars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'long', 'when', 'get', 'is', 'paypal', 'someth', 'you', 'a', 'hey', 'my', 'good', 'day', 'me', 'with', 'thank', 'have', 'mastercard', 'tell', 'accept', 'of', 'know', 'hi', 'are', 'anyon', 'item', 'help', 'do', 'what', 'hello', 'doe', 'ship', 'there', 'bye', 'kind', 'onli', 'lot', 'which', 'pay', 'credit', 'goodby', 'later', 'sell', 'card', 'deliveri', 'how', 'that', 'cash', 'see', 'i', 'funni', \"'s\", 'joke', 'take', 'can'}\n"
     ]
    }
   ],
   "source": [
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[\"'s\", 'a', 'accept', 'anyon', 'are', 'bye', 'can', 'card', 'cash', 'credit', 'day', 'deliveri', 'do', 'doe', 'funni', 'get', 'good', 'goodby', 'have', 'hello', 'help', 'hey', 'hi', 'how', 'i', 'is', 'item', 'joke', 'kind', 'know', 'later', 'long', 'lot', 'mastercard', 'me', 'my', 'of', 'onli', 'pay', 'paypal', 'see', 'sell', 'ship', 'someth', 'take', 'tell', 'thank', 'that', 'there', 'what', 'when', 'which', 'with', 'you']\n54\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(all_words)\n",
    "print(all_words)\n",
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['delivery', 'funny', 'goodbye', 'greeting', 'items', 'payments', 'thanks']\n"
     ]
    }
   ],
   "source": [
    "tags = sorted(set(tags))\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "X_train = []    # contains all patterns \n",
    "Y_train = []    # contains all tags or categories\n",
    "for x in pattern_tags:\n",
    "    bag = bag_words(x[0], all_words)   # sentence, all_words\n",
    "    X_train.append(bag)\n",
    "    label = tags.index(x[1])\n",
    "    Y_train.append(label)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train, dtype=np.int64)\n",
    "print(type(X_train))\n",
    "# X_train = X_train.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'torch.Tensor'>\n<class 'torch.Tensor'>\ntensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 1.],\n        ...,\n        [0., 1., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 1., 0.,  ..., 0., 0., 1.]])\n<class 'torch.Tensor'>\ntensor([3, 3, 3, 3, 3, 3, 2, 2, 2, 6, 6, 6, 6, 4, 4, 4, 5, 5, 5, 5, 0, 0, 0, 1,\n        1, 1])\n"
     ]
    }
   ],
   "source": [
    "class ChatDataSet(Dataset):\n",
    "    def __init__(self):\n",
    "        self.n_samples = len(X_train)\n",
    "        self.x_data = torch.from_numpy(X_train)\n",
    "        self.y_data = torch.from_numpy(Y_train)\n",
    "        print(type(self.x_data))\n",
    "        print(type(self.x_data[0]))\n",
    "        print(self.x_data)\n",
    "        print(type(self.y_data))\n",
    "        print(self.y_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "dataset = ChatDataSet()\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True) # entire training data is contained in this 1 object now\n"
   ]
  },
  {
   "source": [
    "## Defining the neural network structure"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.l3 = nn.Linear(hidden_size, classes)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "source": [
    "# <img src=\"pics/structure.jpg\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "54 54\n7 ['delivery', 'funny', 'goodbye', 'greeting', 'items', 'payments', 'thanks']\n"
     ]
    }
   ],
   "source": [
    "INPUT_SIZE = len(X_train[0])\n",
    "OUTPUT_SIZE = len(tags)\n",
    "HIDDEN_SIZE = 8\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "LEARN_RATE = 0.001\n",
    "EPOCHS = 1000\n",
    "\n",
    "print(INPUT_SIZE, len(all_words))\n",
    "print(OUTPUT_SIZE, tags)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Model(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE).to(device)"
   ]
  },
  {
   "source": [
    "## Training the network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# <img src=\"pics/logloss.jpg\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 100 / 1000, loss: 1.2412502765655518 \n",
      "epoch: 200 / 1000, loss: 0.08783357590436935 \n",
      "epoch: 300 / 1000, loss: 0.016463223844766617 \n",
      "epoch: 400 / 1000, loss: 0.015537867322564125 \n",
      "epoch: 500 / 1000, loss: 0.006050540134310722 \n",
      "epoch: 600 / 1000, loss: 0.004362071864306927 \n",
      "epoch: 700 / 1000, loss: 0.0003328182501718402 \n",
      "epoch: 800 / 1000, loss: 0.0006490105297416449 \n",
      "epoch: 900 / 1000, loss: 0.0023323972709476948 \n",
      "epoch: 1000 / 1000, loss: 0.001341348048299551 \n",
      "final loss: 0.001341348048299551\n"
     ]
    }
   ],
   "source": [
    "# Optimization \n",
    "\n",
    "xEntropyLoss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARN_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for (words, labels) in train_loader:\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        \n",
    "        outputs = model(words) \n",
    "            \n",
    "        loss = xEntropyLoss(outputs, labels)\n",
    "\n",
    "        #Back propogation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"epoch: {epoch+1} / {EPOCHS}, loss: {loss.item()} \")\n",
    "\n",
    "\n",
    "print(f\"final loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'model_state': model.state_dict(),\n",
    "    'input_size': INPUT_SIZE,\n",
    "    'output_size': OUTPUT_SIZE,\n",
    "    'hidden_size': HIDDEN_SIZE,\n",
    "    'all_words': all_words,\n",
    "    'tags': tags\n",
    "}\n",
    "\n",
    "file = 'data.pth'\n",
    "torch.save(data, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}