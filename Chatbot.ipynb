{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37432bit0bab2797a6d74ec0b66b64f8d195f925",
   "display_name": "Python 3.7.4 32-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "portStemmer = PorterStemmer()\n",
    "def tokenize(sentence):\n",
    "    return nltk.word_tokenize(sentence)\n",
    "\n",
    "\n",
    "def stem(word):    \n",
    "    return portStemmer.stem(word.lower())\n",
    "\n",
    "\n",
    "def bag_words(pattern_stentence, all_words):\n",
    "    sentence = [stem(w) for w in pattern_stentence]\n",
    "    bag = np.zeros(len(all_words), dtype=np.float32)\n",
    "    for i, w in enumerate(all_words):\n",
    "        if w in sentence:\n",
    "            bag[i] = 1.0\n",
    "    return bag\n",
    "    # for word in all_words:\n",
    "    #     if word in sentence:\n",
    "    #         bag.append(1.0)\n",
    "    #     else:\n",
    "    #         bag.append(0.0)\n",
    "    # return np.array(bag, dtype=np.float32)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1., 0., 1., 1.], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 180
    }
   ],
   "source": [
    "sentence = [\"WHat\", \"the\", \"fuck\", \"is\", \"going\", \"on\"]\n",
    "words = [\"fuck\", \"bye\", \"what\", \"on\"]\n",
    "bag_words(sentence, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"categories.json\", \"r\") as f:\n",
    "    intents = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "tags = []\n",
    "pattern_tags = []\n",
    "for intent in intents[\"intents\"]:\n",
    "    tag = intent[\"tag\"]\n",
    "    tags.append(tag)\n",
    "    pattern_tag = []\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        tokenized_pattern = tokenize(pattern)\n",
    "        all_words.extend(tokenized_pattern)\n",
    "        pattern_tags.append((tokenized_pattern, tag))\n",
    "\n",
    "ignore_chars = [\"?\", \"!\", \".\", \",\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Hi', 'Hey', 'How', 'are', 'you', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Bye', 'See', 'you', 'later', 'Goodbye', 'Thanks', 'Thank', 'you', 'That', \"'s\", 'helpful', 'Thank', \"'s\", 'a', 'lot', '!', 'Which', 'items', 'do', 'you', 'have', '?', 'What', 'kinds', 'of', 'items', 'are', 'there', '?', 'What', 'do', 'you', 'sell', '?', 'Do', 'you', 'take', 'credit', 'cards', '?', 'Do', 'you', 'accept', 'Mastercard', '?', 'Can', 'I', 'pay', 'with', 'Paypal', '?', 'Are', 'you', 'cash', 'only', '?', 'How', 'long', 'does', 'delivery', 'take', '?', 'How', 'long', 'does', 'shipping', 'take', '?', 'When', 'do', 'I', 'get', 'my', 'delivery', '?', 'Tell', 'me', 'a', 'joke', '!', 'Tell', 'me', 'something', 'funny', '!', 'Do', 'you', 'know', 'a', 'joke', '?']\n"
     ]
    }
   ],
   "source": [
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying stemming and remove duplicate words by type-casting the list into a set\n",
    "all_words = set([stem(w) for w in all_words if w not in ignore_chars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'can', 'do', 'good', 'hi', 'what', 'tell', 'someth', 'know', 'hey', 'day', 'with', 'of', 'later', 'have', 'i', 'goodby', 'which', 'joke', 'get', 'deliveri', 'mastercard', 'cash', 'onli', 'when', 'help', 'card', 'item', \"'s\", 'thank', 'accept', 'sell', 'pay', 'bye', 'credit', 'is', 'long', 'a', 'how', 'that', 'lot', 'doe', 'there', 'anyon', 'see', 'take', 'kind', 'my', 'funni', 'ship', 'are', 'you', 'me', 'hello', 'paypal'}\n"
     ]
    }
   ],
   "source": [
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[\"'s\", 'a', 'accept', 'anyon', 'are', 'bye', 'can', 'card', 'cash', 'credit', 'day', 'deliveri', 'do', 'doe', 'funni', 'get', 'good', 'goodby', 'have', 'hello', 'help', 'hey', 'hi', 'how', 'i', 'is', 'item', 'joke', 'kind', 'know', 'later', 'long', 'lot', 'mastercard', 'me', 'my', 'of', 'onli', 'pay', 'paypal', 'see', 'sell', 'ship', 'someth', 'take', 'tell', 'thank', 'that', 'there', 'what', 'when', 'which', 'with', 'you']\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(all_words)\n",
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['delivery', 'funny', 'goodbye', 'greeting', 'items', 'payments', 'thanks']\n"
     ]
    }
   ],
   "source": [
    "tags = sorted(set(tags))\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "Y_train = []\n",
    "for x in pattern_tags:\n",
    "    bag = bag_words(x[0], all_words)   # sentence, all_words\n",
    "    X_train.append(bag)\n",
    "    label = tags.index(x[1])\n",
    "    Y_train.append(label)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train, dtype=np.int64)\n",
    "print(type(X_train))\n",
    "# X_train = X_train.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'torch.Tensor'>\n<class 'torch.Tensor'>\ntensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 1.],\n        ...,\n        [0., 1., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 1., 0.,  ..., 0., 0., 1.]])\n<class 'torch.Tensor'>\ntensor([3, 3, 3, 3, 3, 3, 2, 2, 2, 6, 6, 6, 6, 4, 4, 4, 5, 5, 5, 5, 0, 0, 0, 1,\n        1, 1])\n"
     ]
    }
   ],
   "source": [
    "class ChatDataSet(Dataset):\n",
    "    def __init__(self):\n",
    "        self.n_samples = len(X_train)\n",
    "        self.x_data = torch.from_numpy(X_train)\n",
    "        self.y_data = torch.from_numpy(Y_train)\n",
    "        print(type(self.x_data))\n",
    "        print(type(self.x_data[0]))\n",
    "        print(self.x_data)\n",
    "        print(type(self.y_data))\n",
    "        print(self.y_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "dataset = ChatDataSet()\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.l3 = nn.Linear(hidden_size, classes)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "54 54\n7 ['delivery', 'funny', 'goodbye', 'greeting', 'items', 'payments', 'thanks']\n"
     ]
    }
   ],
   "source": [
    "INPUT_SIZE = len(X_train[0])\n",
    "OUTPUT_SIZE = len(tags)\n",
    "HIDDEN_SIZE = 8\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "LEARN_RATE = 0.001\n",
    "EPOCHS = 1000\n",
    "\n",
    "print(INPUT_SIZE, len(all_words))\n",
    "print(OUTPUT_SIZE, tags)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Model(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 100 / 1000, loss: 1.0150247812271118 \n",
      "epoch: 200 / 1000, loss: 0.1493098884820938 \n",
      "epoch: 300 / 1000, loss: 0.00857056025415659 \n",
      "epoch: 400 / 1000, loss: 0.0026586707681417465 \n",
      "epoch: 500 / 1000, loss: 0.0016610106686130166 \n",
      "epoch: 600 / 1000, loss: 0.0007009432883933187 \n",
      "epoch: 700 / 1000, loss: 0.00014756870223209262 \n",
      "epoch: 800 / 1000, loss: 0.0020305849611759186 \n",
      "epoch: 900 / 1000, loss: 0.0004318160645198077 \n",
      "epoch: 1000 / 1000, loss: 0.0007990890881046653 \n",
      "final loss: 0.0007990890881046653\n"
     ]
    }
   ],
   "source": [
    "# Optimization \n",
    "\n",
    "xEntropyLoss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARN_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for (words, labels) in train_loader:\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        \n",
    "        outputs = model(words) \n",
    "            \n",
    "        loss = xEntropyLoss(outputs, labels)\n",
    "\n",
    "        #Back propogation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"epoch: {epoch+1} / {EPOCHS}, loss: {loss.item()} \")\n",
    "\n",
    "\n",
    "print(f\"final loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}